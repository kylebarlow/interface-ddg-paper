%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Start the main part of the manuscript here.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Protein-protein interactions underlie essentially all biological processes, including signal transduction and antibody-antigen recognition. Many protein-protein interfaces are sensitive to mutations that can alter interaction affinity and specificity.
In fact, mutations at protein-protein interfaces have been reported to be over-represented within disease-causing mutations\cite{jubb_mutations_2017}, indicating the central importance of these interactions to biology and human health.
A sufficiently accurate computational method capable of predicting mutations that strengthen or weaken known protein-protein interactions would hence serve as a useful experimental tool to dissect the role of specific protein-protein interactions in important biology processes. Coupled with state-of-the-art methods for protein engineering and design, such a method would also enhance our ability to create new and selective interactions, enabling the development of improved protein therapeutics, protein-based sensors, and protein materials.

Several prior methods have attempted to predict changes in protein-protein binding affinity upon mutation using different approaches to estimating energetic effects (scoring) and modeling structural changes (sampling). Common approaches include
weighted energy functions that seek to describe physical interactions underlying protein-protein interactions\cite{guerois_predicting_2002,kamisetty_accounting_2011},
statistical and contact potentials \cite{dehouck_beatmusic:_2013,moal_intermolecular_2013,vangone_contacts-based_2015,brender_predicting_2015},
a combination of these approaches\cite{li_predicting_2014},
graph-based representations\cite{pires_mcsm:_2014},
and methods that attempt to sample backbone structure space locally around mutations\cite{dourado_multiscale_2014}.

We set out to develop and assess methods for prediction of change in binding free energy after mutation (interface \ddg) within the Rosetta macromolecular modeling suite. Rosetta is freely available for academic usage, allowing combination of these predictions with Rosetta's powerful protein design capabilities, which have proven successful in a variety of applications \cite{kaufmann_practically_2010}. % TODO cite more reviews?
Prior projects have applied Rosetta predictions to
dissect determinants of binding specificity and promiscuity\cite{boulanger_convergent_2003,mcfarland_symmetry_2003},
enhance protein-protein binding affinities \cite{sammond_structure-based_2007,song_rational_2006},
and to design modified\cite{kortemme_computational_2004,kapp_control_2012}
and new interactions\cite{chevalier_design_2002,fleishman_computational_2011,chevalier_massively_2017}, but no prior benchmarking effort has studied the performance of predicting changes in binding free energy in Rosetta on a large, diverse benchmark data set, in part because such a dataset has only become available more recently.
The current state-of-the-art Rosetta \ddg\ method,  ddg\_monomer \cite{kellogg_role_2011}, has proven effective at predicting changes in stability in monomeric proteins after mutation, but had not yet been tested at predicting change of binding free energies in protein-protein complexes.
Prior ``computational alanine scanning'' \ddg\ methods were benchmarked on mutations in protein-protein interfaces, focusing on mutations to alanine \cite{kortemme_simple_2002,kortemme_computational_2004,conchuir_web_2015}.
The original alanine scanning method sampled only side chain degrees of freedom, which is a first-order approximation for mutations to alanine (which are not expected to cause large backbone perturbations\cite{cunningham_high-resolution_1989}), but less likely to be predictive for mutations to larger side chains which might require some degree of backbone rearrangement to accommodate the change.
Adaptation of the alanine scanning method to recent energy function and sampling method developments in Rosetta has not shown improvement in benchmarking\cite{conchuir_web_2015}, indicating a need to more thoroughly develop and test a method that attempts to more aggressively sample conformational space.

We sought to create a method that would take into account aspects of the natural conformational plasticity of proteins by representing structures as an ensemble of individual full-atom models that would effectively explore the biologically relevant and accessible portions of conformational space close to the native structure.
Ensemble representations have been previously been shown to be effective both to predict changes in protein stabilities after mutation and to predict the effects of mutation on protein-protein binding affinities\cite{benedix_predicting_2009}, as well as to improve $\Delta G_{binding}$ calculations between kinases and their inhibitors \cite{araki_effect_2016}.

We chose to sample conformational diversity using the ``backrub'' protocol implemented in Rosetta \cite{smith_backrub-like_2008}.
The backrub method samples local, coupled, side chain and backbone conformational changes, similar to those observed to underlie conformational heterogeneity in high-resolution crystal structures \cite{davis_backrub_2006}.
Backrub ensembles appear to recapitulate properties of proteins that have been experimentally determined, such as side chain NMR order parameters\cite{friedland_simple_2008}, tolerated sequence profiles at protein-protein \cite{humphris_prediction_2008} and protein-peptide interfaces \cite{smith_structure-based_2010,smith_predicting_2011}, and conformational variability between protein homologs\cite{schenkelberg_protein_2016}.
Backrub has also proved effective in design applications, such as for the redesign of protein-protein interfaces\cite{kapp_control_2012} and recapitulation of mutations that alter ligand-binding specificity\cite{ollikainen_coupling_2015}.
When compared to ensembles generated via molecular dynamics simulations or the “PertMin”method\cite{davey_improving_2014}, backrub ensembles were shown in certain cases to be
the only ensembles with a higher diversity (as measured with RMSD) from each other than from the original input crystal structure, indicating that backrub could be uniquely suited to produce diverse ensembles that effectively explore the local conformational space around the input structure.\cite{davey_improving_2014}
Taken together, we hypothesized that these previously demonstrated properties of backrub ensembles would translate into their effective use as an effective representation of near-native conformational states for use in predicting interface \ddg\ values.

\section{Methods}

Developing and assessing the accuracy of a new method to predict changes in binding free energy after mutbeation requires a large and diverse benchmark set covering single mutations to all amino acid types, multiple mutations, and mutations across a variety of protein-protein interfaces.
To facilitate comparisons to other methods and to avoid biases specific to our approach, we chose to use an existing benchmark dataset created by Dourado and Flores\cite{dourado_multiscale_2014} during the devlopment of their ZEMu (Zone Equilibration of Mutants) method.
The ZEMu dataset was curated from the larger SKEMPI database\cite{moal_skempi:_2012} by avoiding a bias towards complexes in which a single position is repeatedly mutated, experimental data that is not peer-reviewed, redundancy (duplicate experimental values), mutations outside of interfaces, mutations involved in crystal contacts, and experimental \ddg\ values for which wild-type and mutant conditions (such as pH) varied.
Confidence in the ``known'' experimental \ddg\ values is important, as it has been pointed out that the experimental methodology used can have a strong effect on the performance of predictors of changes in binding free energy\cite{geng_exploring_2016}.
The ZEMu dataset was also curated to include a wide range of both stabilizing and destabilizing mutants, small side chain to large side chain mutations, single and multiple mutations, and a diversity of complexes (\cref{tab:table-composition}).

\subimport*{figs-and-tables/}{table-composition}

After a review of the literature from which the known experimental \ddg\ values originated, we removed one data point from the 1254 point ZEMu set that we could not match to the originally reported affinity value. We also removed 5 mutations we determined to be duplicates, along with 8 mutations that were reverse mutations of other data points, leaving us with a test set of 1240 mutations.
We defined complexes that contained at least one antibody binding partner by comparison of PDB identifiers with SAbDab \cite{dunbar_sabdab:_2014}.
Our version of the ZEMu dataset is available in the Supporting Information as \cref{tab:zemu-filtered}.

Our protocol, called ``flex ddG'', is implemented within the RosettaScripts scripting interface to the Rosetta macromolecular modeling software suite \cite{fleishman_rosettascripts:_2011}, which makes the protocol easily adaptable to future improvements and energy function development. We utilized Rosetta's Talaris \cite{leaver-fay_chapter_2013,song_structure-guided_2011,shapovalov_smoothed_2011} energy function.
Version numbers of tested software are available in \cref{tab:table-versions}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth,keepaspectratio]{figures/fig-overview.pdf}
    \caption[]{
      Schematic of the flex ddG protocol method.
  } \label{fig:figure-overview}
\end{figure}

Flex ddG method steps are outlined in \cref{fig:figure-overview} (full Rosetta Scripts XML available in the Supporting Information).
\textbf{Step 1:} The protocol begins with an initial minimization (on backbone $\phi$/$\psi$\ and side chain $\chi$\ torsional degrees of freedom, using the ``lbfgs\_armijo\_nonmonotone'' minimization algorithm option) of the input model using the Rosetta energy function. This (and later) minimizations are performed with constraints that harmonically restrain pairwise atom distance to their values in the input crystal structure. Minimization is run until convergence is achieved, meaning that the model score prior to minimization is within one REU (Rosetta Energy Unit) of the model post-minimization.
\textbf{Step 2:} Starting from the minimized input structure, the backrub method in Rosetta is used to create an ensemble of models. In brief, each backrub move is undertaken on a randomly chosen three-residue stretch of protein, where the middle residue's neighbor atom is within 8 \AA\ of any mutant position. The neighbor atom used for distance calculations is each residue's C-$\beta$ atom, except in the case of glycine, which uses C-$\alpha$. All atoms in the three-residue stretch of structure are rotated locally about an axis defined as the vector between the endpoint C-$\alpha$ atoms. Backrub is run at a temperature of 1.2, for up to 60,000 backrub Monte Carlo trials. Up to 50 output structures are generated.
\textbf{Step 3A:} For each of the 50 structure models in the ensemble (output by backrub), the Rosetta ``packer'' is used to optimize side chain conformations for the wild-type sequence using discete rotameric conformations \cite{shapovalov_smoothed_2011}. The packer is run with the multi-cool annealer option\cite{leaver-fay_generic_2011}, which is set to keep a history of the 6 best rotameric states visited during annealing.
\textbf{Step 3B:} Independently and in parallel to step 3A, side chain conformations for the mutant sequence are optimized on all 50 models.
\textbf{Step 4A:} Each of the 50 wild-type structures is minimized, again adding pairwise atom-atom constraints to the input structure. Minimization is run with the same parameters as in step 1; the coordinate constraints used in this step are taken from the coordinates of the Step 3A structure.
\textbf{Step 4B:} In the same fashion as in Step 4A, but for each of the 50 mutant models.
\textbf{Step 5A:} Each of the 50 minimized wild-type structures are scored in complex, and the individual complex components are scored individually. The scores of the split, unbound complex components are obtained simply by splitting the complex halves away from each other. No further minimization or side chain optimization is performed on the unbound states before scoring.
\textbf{Step 5B:} In the same fashion as Step 5A, each of the 50 minimized mutant structures are scored in complex, and the individual complex components are scored individually.
\textbf{Step 6:} The interface \ddg\ score is produced via Eq. \ref{eqn:split-ddg}:

\begin{equation}\label{eqn:split-ddg}
  \begin{split}
    {\Delta\Delta}G_{bind} & ={\Delta}G^{MUT}_{bind} - {\Delta}G^{WT}_{bind}\\
    & =({\Delta}G^{MUT}_{complex} - {\Delta}G^{MUT}_{partner A} - {\Delta}G^{MUT}_{partner B})\\
    & \quad - ({\Delta}G^{WT}_{complex} - {\Delta}G^{WT}_{partner A} - {\Delta}G^{WT}_{partner B})\\
  \end{split}
\end{equation}

We evaluate performance of the protocol by comparing predicted \ddg\ scores to known experimental values, using Pearson's correlation R, Mean Absolute Error (MAE), and Fraction Correct (FC). Fraction Correct is defined as the number of mutations categorized correctly as stabilizing, neutral, or destabilizing, divided by the total number of mutations in the benchmark set. Stabilizing mutations are defined as those with a \ddg\ $<=$\ -1.0 kcal/mol, neutral as those with -1.0 kcal/mol $<$\ \ddg\ $<$\ 1.0 kcal/mol, and destabilizing as those with \ddg\ $>=$\ 1.0 kcal/mol. Prior to calculating fraction correct, \ddg\ values predicted by Rosetta in REU (Rosetta Energy Units) are scaled into kcal/mol based on the linear fit of predicted to experimental values on the entire benchmark set.

MAE (Mean Absolute Error) is defined in Eq.~\ref{eqn:mae} as:

\begin{equation}\label{eqn:mae}
  MAE = \dfrac{1}{n}\sum\limits_{i=1}^n|y_i-x_i| = \dfrac{1}{n}\sum\limits_{i=1}^n|e_i|
\end{equation}

where $y_i$ are the predicted \ddg\ values, $x_i$ are the known, experimentally determined values, and $e_i$ is the prediction error.

\section{Results and discussion}

\subimport*{figs-and-tables/}{table-main}

The overall performance of the protocol is summarized in \cref{tab:table-main}.
We compare 4 prediction methods: (a) our flex ddG backrub ensemble method, (b) the prior state-of-the-art Rosetta methodology, ddg\_monomer \cite{kellogg_role_2011}, (c) a control version of our flex ddG protocol which omits the backrub ensemble generation step, leaving only the minimization and packing steps, and (d) published data from the ZEMu (zone equilibration of mutants) method\cite{dourado_multiscale_2014}.

The new flex ddG method outperforms the comparison methods on the complete dataset in each of the correlation, MAE, and fraction correct metrics (Table 2). In particular, we see a large increase in performance relative to the other methods on the small-to-large subset of mutations. This is in accordance with our expectations described above that backrub ensembles should be able to sample small backbone conformational adjustments required to accommodate changes in amino acid residue size. Notably, application of backrub ensembles performs better than other methods that include backbone minimization steps, including the current state-of-the-art Rosetta ddg\_monomer method that on the Small-to-Large Mutations subset achieves a Pearson correlation of only 0.31 compared to 0.64 with flex ddg.
Performance of the flex ddG on the subset of single mutations to alanine is also competitive or outperforms the alternative methods.
As we do not expect single mutations to alanine to require intensive backbone sampling, our method's effectiveness on this subset shows that it is fairly robust to the mutation type.
This observation could be explained by the fact that we undertake backrub sampling prior to making the mutation to sample underlying, relevant plasticity of the input crystal structure instead of distorting the local structure around a mutation to resolve a clash or poor interaction with a mutant side chain.
Finally, our method shows improved performance compared to the control method and ddg\_monomer on the subset of multiple mutations, but for this set does not match the performance of the ZEMu method.
This could indicate that further refinement to the backrub sampling parameters is required in the case of multiple mutations.
As there are more mutation sites, there will be more surrounding backrub pivot residue sites.
However, flex ddG outperforms ZEMu on multiple mutations if none of the mutations are to alanine (\cref{tab:table-main}).

The underlying scatterplots for the flex ddG and control methods on the complete dataset and small-to-large subsets are shown in \cref{fig:figure-scatter}. A notable improvement with flex ddG over the control can be seen for mutations that were experimentally determined to stabilize the protein-protein interface (\ddg\ $<=$\ -1.0 kcal/mol). As above, this improvement is particularly visible for the Small-to-Large Mutations subset. For this set, the control method misclassifies most stabilizing mutations to have no effect or even be destabilizing, whereas flex ddG identifies a sizeable number correctly as stabilizing. This capability is especially important for challenging design applications to modulate binding affinity as well as selectivity, as well as creating entirely new high-affinity protein-protein interactions.

In the following sections, we set out to assess how different flex ddG implementations would affect its prediction performance.

\subimport*{figs-and-tables/}{figure-scatter}

\subimport*{figs-and-tables/}{structs-v-corr-WildTypeComplex-zemu-12-60000-rscript-validated-t14}

\subsection{Effect of ensemble size}

The results presented above used an ensemble size of 50 members but it is unclear what the optimal ensemble size should be. For example, prior methods used ensemble sizes ranging from 10\cite{kamisetty_accounting_2011}\ to thousands\cite{benedix_predicting_2009}. As here the computational time increases linearly with ensemble size, determining an optimal size is practically relevant. We therefore evaluated the performance of flex ddG as we average across an increasing number of structures (from 1 to 50,\cref{fig:structs-v-corr-WildTypeComplex-zemu-12-60000-rscript-validated-t14}.
The structures used are first sorted by the score of the corresponding repacked and minimized wild type model, such that producing a \ddg\ with 1 model will only use the lowest (best) scoring model, 2 models will use the 2 lowest scoring models, and so forth.
\cref{fig:structs-v-corr-WildTypeComplex-zemu-12-60000-rscript-validated-t14}(a) shows performance on the complete dataset.
As more structures with increasing wild type complex score are averaged, correlation with known experimental values increases.
Conversely, performance for the no backrub control method (shown in light blue) decreases slightly as more structures are averaged.
This result indicates that sampling with backrub adds information that improves \ddg\ calculation, despite the additional structures being used having higher scores (\cref{fig:wildtypecomplex-scores-complete}).
These higher scoring models would typically be excluded in other methods such as the Rosetta ddg\_monomer protocol, which averages the predictions from the best XX models.

In contrast to the published use of the ddg\_monomer protocol\cite{kellogg_role_2011}, we find that the performance of the ddg\_monomer method also improves as more output structures are averaged (\cref{fig:structs-v-corr-WildTypeComplex-ddg-monomer-16-003-zemu-2}).
This was somewhat unexpected, as the no-backrub control method, which did not show an improvement with increasing ensemble size, is conceptually similar to the ddg\_monomer method. However, the difference may arise from the fact that the ddg\_monomer method ramps the repulsive term of the energy function during minimization. This strategy explores conformational space more broadly in different backbone ensemble members than minimization with a fully weighted repulsive term in the no-backrub control method. In this fashion, including more ensemble members generated by the ddg\_monomer method increases the conformational plasticity sampled which in turn increases performance, as seen for the flex ddg method.

The subset of small-to-large mutations shows the largest increase in correlation with experimental \ddg\ values as more structures are averaged (\cref{fig:structs-v-corr-WildTypeComplex-zemu-12-60000-rscript-validated-t14}(b)). This is consistent with our reasoning above that improved modeling of conformational plasticity is important for prediction performance, and that this effect is most important for significant changes in amino acid residue size. However, the subset of multiple mutations where none are mutations to alanine shown in \cref{fig:structs-v-corr-WildTypeComplex-zemu-12-60000-rscript-validated-t14}(c) does not show monotonically increasing performance as more structures are averaged. This observation indicates limitations of backrub sampling, where modeling of larger conformational changes might be necessary for multiple mutations.

Averaging across increased numbers of structures also improves correlation for the subset of single mutations to alanines (\cref{fig:structs-v-corr-WildTypeComplex-zemu-12-60000-rscript-validated-t14}(d)). This observation indicates that increased sampling, in the very least, is not harmful for cases where one would expect structural changes to be relatively small on average.

From a practical standpoint, simply generating 20-30 structures should constitute sufficient sampling for most cases, as the performance when selecting the best scoring 20 out of 50 models is not significantly improved over the results in \cref{fig:structs-v-corr-id-zemu-12-60000-rscript-validated-t14} (where there is no sorting of structures by score).

\subsection{Effect of changing backrub sampling steps}

Sampling can also be controlled by changing the length of the backrub simulation, as measured in the number of Monte Carlo sampling steps.
\cref{fig:steps-v-corr} shows the effect of increasing backrub simulation length (while averaging all 50 structures at each output step) on performance.
\ddg\ scores are calculated every 2,500 backrub steps (shown in circles for correlation and in squares for MAE).
A ``X'' marks the performance with zero backrub steps (control minimization and side chain packing only method).

As we observed when averaging over more structures, increased performance is also seen in both correlation with experimental data and MAE as backrub simulation length increases for the subsets of small-to-large mutations (panel b) and multiple mutations, none to alanine (panel c).
Performance reaches a maximum at 30,000 backrub steps, after which it levels off.
Performance improves for the first step of 2500 backrub steps on the complete dataset and for single mutations to alanine, but remains relatively flat afterwards.

The increased performance seen is not simply a result of scores decreasing as the simulation progresses, as the score of the minimized wild type complex does not decrease uniformly across the sampled ensemble as the simulation progresses (\cref{fig:wildtypecomplex-scores-complete}).
\cref{fig:t14-mean-ensemble} shows that pairwise backrub ensemble RMSDs continue to increase throughout the backrub simulation for all subsets, indicating that diminishing returns at 30,000+ steps is not a result of failure to sample new states, but rather might indicate that no additional sampling is needed to capture the degree of local change in structure that occurs post-mutation in this benchmark set.
\cref{fig:t14-mean-ensemble} also shows that the starting pairwise ensemble RMSDs vary for each different subset, reflecting the fact that different subsets are composed of different wild-type complexes with different inherent flexibility (as sampled by backrub).
Correspondingly, \ddg\ predictive power also varies depending on the input complex, as showin in \cref{tab:table-by-structure}.
This different inherent flexibility should be kept in mind when comparing results across subsets.

Unlike when increasing the number of averaged structures, we see continual improved performance with additional sampling (from longer backrub simulations) on the subset of multiple mutations (not to alanine).
This indicates that sampling cannot be simply ``increased'' by independently tuning either the length of the backrub simulation or the number of models generated. % TODO Tanja said to take this out, but maybe we can say how to tune in the paper? , both parameters are independent and must be tuned separately when running the flex ddG protocol.

\subsection{Score analysis}

As the sampling/scoring problems of protein modeling are inextricably linked, it is often the case that improving one enables further improvements on the other.
% TODO (CITE PAPERS: I think this is mentioned in Dan's KIC paper, in a loop sampling paper from Rhiju, and a paper with Mike Tyka). # Kyle couldn't find this
We sought to analyze underlying errors of the Rosetta score function (when applied to interface \ddg) by reweighting the generally parameterized energy function for this specific application.
Additionally, analyzing a reweighted energy function on a score term by score term basis could provide insight into which terms might benefit from future developments.

We chose to reweight the energy function using a non-linear reweighting scheme similar to Generalized Additive Models (GAMs)\cite{wood_fast_2011}.
In this reweighting method, we use Monte Carlo sampling to fit either a linear transformation or a sigmoid function to the individual distributions of score terms, with the objective function of reducing the absolute error between our predictions and known experimental values over the entire dataset.

As we do not modify our models of the unbound state, any effects on stability of the complex partners will cancel out, as the $\Delta G$\ of folding score of the unbound partners is subtracted from the total score of the complex (\cref{eqn:split-ddg}).
Not modeling conformational change in the unbound models might be effective because to model any such fluctuation might produce more noise than signal when the scores for the bound and unbound states are subtracted.
This is supported by the prior observation that the mobility of amino acids at dimeric interfaces is generally lower than for other amino acids at the protein surface exposed to solvent \cite{zen_comparing_2010}.

The terms in the Rosetta Talaris energy function that cancel out to zero are: yhh\_planarity, pro\_close, hbond\_sr\_bb, ref, fa\_dun, fa\_intra\_rep, omega, p\_aa\_pp, and rama.
Seven score terms are left; combined they become the final interface \ddg\ score: fa\_sol, hbond\_sc, hbond\_bb\_sc, fa\_rep, fa\_elec, hbond\_lr\_bb and fa\_atr.

The fit sigmoid and linear functions are shown in \cref{fig:t14-fits-feats}.
The effect on the distribution of predictions is shown in \cref{fig:t14-fit-scatter}.

\section{Conclusions}

We have shown that our new ``flex ddG'' method for estimating change in binding affinity after mutation in protein-protein interfaces is more effective than previous methods on a large, curated benchmark dataset.
Particular improvement in performance is seen on the subset of small-to-large mutations, indicating that modeling backbone flexibility does improve performance in the case where backbone rearrangements are expected to be more common.

We have also shown more accurate predictions can be obtained by averaging the scores across a generated ensemble of backrub microstates, and that the number of required states is relatively low (20-30).
Prior methods that attempted to produce \ddg\ predictions by averaging an ensemble of models required on the order of thousands of structures  \cite{benedix_predicting_2009}, indicating that backrub sampling can efficiently sample the local conformational landscape around a wild-type structures that is relevant for interface \ddg\ prediction.

By creating a method that uses backrub to sample conformational space more broadly than minimization alone can sample, while still staying close in conformational space to the known wild-type input structure, we have also generated data that should prove useful for future energy function improvements.
In particular, performance with Rosetta's newest REF energy function\cite{alford_rosetta_2017} is currently not better in our method than performance with the prior Talaris energy function (\cref{tab:table-ref}), indicating that the backrub sampling parameters might require further benchmarking and adaption to the REF energy function.
Our error analysis via GAM-like reweighting also indicates potential score function improvement could be obtained via non-linear score term reweighting, and that examination of the weights we obtained for interface \ddg\ prediction might provide insight into why the current energy function fails on certain cases in our dataset.
Further improvements might also be obtained by more explicitly including the effects of entropy, including the potential to use our ensembles to calculate change in conformational entropy after mutation.

% In the future, we might need to look more at structure metrics.
% Why do our backrub ensembles work better than \cite{kamisetty_accounting_2011} and \cite{kellogg_role_2011}, which also used backrub exactly (GOBLIN) or something like it (Kellogg). Where the sampling takes place, and how much, are important. We've focused around mutations.
% Why no boltzmann weighting? If we are sampling the underlying distribution unbiased, we wouldn't need to.
% Entropy.

% \begin{itemize}
% \item Monomeric Rosetta ddG does not work for interfaces
% \item As stated in intro, ensembles have advantages, etc.
% \item Dataset: zemu (why better than skempi). Table 1
% \item General prediction protocol in figure 1
% \item Metric description: pearson’s R, fraction correct, MAE
% \item Description of main results, subsets, and comparison to zemu method
% \item Scatter plots and table
% \item Number of structures in ensemble average, discuss filtering structures by score (ref supp. fig). Fig 4 - one panel showing number of structures effect on correlation (at best backrub step)
% \item Structure comparison - RMSD deviation are subtle. Torsions could be more informative?
% \item We applied machine learning to our cases to try and study which individual score terms were informative
% \end{itemize}

% \subsection{more unfinished}
% Local conformational sampling

% Perhaps because
% As the protein folding funnel is narrower near the free energy minimum {Dill, From Levinthal to pathways to funnels (include this?)}, it should be more possible to find a discrete number of states to represent in an ensemble and use in ddG modeling,
% We can capture the thin part of the funnel:
% “backrub sampling may capture a sizable fraction of localized conformational changes that occur in proteins” \cite{humphris_prediction_2008}

% Questions:
% Shouldn’t it be easier to predict interface ddGs, as we don’t need to consider the stability effects of mutants in the unfolded state
